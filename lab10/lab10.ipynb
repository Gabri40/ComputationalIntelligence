{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "- Submission: Sunday, December 17 ([CET](https://www.timeanddate.com/time/zones/cet))\n",
    "- Reviews: Dies Natalis Solis Invicti ([CET](https://en.wikipedia.org/wiki/Sol_Invictus))\n",
    "\n",
    "Notes:\n",
    "\n",
    "- Reviews will be assigned on Monday, December 4\n",
    "- You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import platform\n",
    "from os import system\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tic Tac Toe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros((3, 3))\n",
    "        self.current_player = 1  # 1 for X  -1 for O\n",
    "\n",
    "    def print_board(self):\n",
    "        if 'windows' in platform.system().lower():\n",
    "            system('cls')\n",
    "        else:\n",
    "            system('clear')\n",
    "        for row in self.board:\n",
    "            for cell in row:\n",
    "                if cell == 1:\n",
    "                    print(\"X\", end=\" \")\n",
    "                elif cell == -1:\n",
    "                    print(\"O\", end=\" \")\n",
    "                else:\n",
    "                    print(\"-\", end=\" \")\n",
    "            print()\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((3, 3))\n",
    "        self.current_player = 1\n",
    "\n",
    "    def is_board_full(self):\n",
    "        return not any(0 in row for row in self.board)\n",
    "\n",
    "    def is_winner(self, player):\n",
    "        return (\n",
    "            np.any(np.all(self.board == player, axis=0))\n",
    "            or np.any(np.all(self.board == player, axis=1))\n",
    "            or np.all(np.diag(self.board) == player)\n",
    "            or np.all(np.diag(np.fliplr(self.board)) == player)\n",
    "        )\n",
    "\n",
    "    def is_game_over(self):\n",
    "        return self.is_winner(1) or self.is_winner(-1) or self.is_board_full()\n",
    "\n",
    "    def is_tie(self):\n",
    "        return self.is_board_full() and not self.is_winner(1) and not self.is_winner(-1)\n",
    "\n",
    "    def get_available_moves(self):\n",
    "        return np.argwhere(self.board == 0)\n",
    "\n",
    "    def make_move(self, move):\n",
    "        self.board[move[0], move[1]] = self.current_player\n",
    "        self.current_player *= -1\n",
    "\n",
    "    def make_random_move(self):\n",
    "        available_moves = self.get_available_moves()\n",
    "        random_move = available_moves[np.random.randint(len(available_moves))]\n",
    "        self.make_move(random_move)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimaxPlayer:\n",
    "    def __init__(self, player_index) -> None:\n",
    "        self.player_index = player_index\n",
    "\n",
    "    def evaluate(self, game) -> int:\n",
    "        if game.is_winner(self.player_index):\n",
    "            return 1\n",
    "        elif game.is_winner(-self.player_index):\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def choose_move(self, game):\n",
    "        if game.is_board_full():\n",
    "            return None\n",
    "\n",
    "        l = len(game.get_available_moves())\n",
    "        if l == 9:\n",
    "            return (np.random.randint(3), np.random.randint(3))\n",
    "\n",
    "        maximizing = self.player_index == game.current_player\n",
    "        best_move = None\n",
    "        best_score = None\n",
    "\n",
    "        for move in game.get_available_moves():\n",
    "            game_copy = deepcopy(game)\n",
    "            game_copy.make_move(move)\n",
    "            score = self.minimax(game_copy, maximizing)\n",
    "            if best_score is None or score > best_score:\n",
    "                best_score = score\n",
    "                best_move = move\n",
    "        return best_move\n",
    "\n",
    "    def minimax(self, game, maximizing):\n",
    "        if game.is_game_over():\n",
    "            return self.evaluate(game)\n",
    "\n",
    "        best_score = None\n",
    "        for move in game.get_available_moves():\n",
    "            game.make_move(move)\n",
    "            score = self.minimax(game, not maximizing)\n",
    "            if best_score is None:\n",
    "                best_score = score\n",
    "            elif maximizing:\n",
    "                best_score = max(best_score, score)\n",
    "            else:\n",
    "                best_score = min(best_score, score)\n",
    "        return best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:07<00:00, 142.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimax is player 1 -> 728 wins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:11<00:00, 89.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimax is player -1 -> 716 wins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "game=TicTacToe()\n",
    "\n",
    "for MINIMAX_INDEX in [1,-1]:\n",
    "    minimax_player=MinimaxPlayer(MINIMAX_INDEX)\n",
    "    wins=0\n",
    "    for _ in tqdm(range(1000)):\n",
    "        game.reset()\n",
    "        while not game.is_game_over():\n",
    "            if game.current_player==MINIMAX_INDEX:\n",
    "                move=minimax_player.choose_move(deepcopy(game))\n",
    "                game.make_move(move)\n",
    "            else:\n",
    "                game.make_random_move()\n",
    "        if game.is_winner(MINIMAX_INDEX):\n",
    "            wins+=1\n",
    "    print(f\"minimax is player {MINIMAX_INDEX} -> {wins} wins\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[218], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         reward\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 52\u001b[0m     \u001b[43mqlearning_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_game_over\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     state\u001b[38;5;241m=\u001b[39mnext_state\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[218], line 31\u001b[0m, in \u001b[0;36mQLearningAgent.learn\u001b[0;34m(self, state, action, reward, next_state, next_action, done)\u001b[0m\n\u001b[1;32m     29\u001b[0m     td_target \u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m     td_target \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_q_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_q_value(state, action, q_value \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m (td_target \u001b[38;5;241m-\u001b[39m q_value))\n",
      "Cell \u001b[0;32mIn[218], line 12\u001b[0m, in \u001b[0;36mQLearningAgent.get_q_value\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_table:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_table[state] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_table[state][\u001b[43maction\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, action[\u001b[38;5;241m1\u001b[39m]]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "class QLearningAgent():\n",
    "    def __init__(self, player_index, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.player_index = player_index\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.q_table = {}\n",
    "\n",
    "    def get_q_value(self, state, action):\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = np.zeros((3, 3))\n",
    "        return self.q_table[state][action[0], action[1]]\n",
    "\n",
    "    def set_q_value(self, state, action, value):\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = np.zeros((3, 3))\n",
    "        self.q_table[state][action[0], action[1]] = value\n",
    "\n",
    "    def choose_action(self, state, game):\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            return game.get_available_moves()[np.random.randint(len(game.get_available_moves()))]\n",
    "        else:\n",
    "            q_values = np.array([self.get_q_value(state, action) for action in game.get_available_moves()])\n",
    "            return game.get_available_moves()[np.argmax(q_values)]\n",
    "\n",
    "    def learn(self, state, action, reward, next_state, next_action, done):\n",
    "        q_value = self.get_q_value(state, action)\n",
    "        if done:\n",
    "            td_target = reward\n",
    "        else:\n",
    "            td_target = reward + self.gamma * self.get_q_value(next_state, next_action)\n",
    "        self.set_q_value(state, action, q_value + self.alpha * (td_target - q_value))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

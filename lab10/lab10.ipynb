{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "- Submission: Sunday, December 17 ([CET](https://www.timeanddate.com/time/zones/cet))\n",
    "- Reviews: Dies Natalis Solis Invicti ([CET](https://en.wikipedia.org/wiki/Sol_Invictus))\n",
    "\n",
    "Notes:\n",
    "\n",
    "- Reviews will be assigned on Monday, December 4\n",
    "- You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros((3, 3))\n",
    "        self.current_player = 1  # 1 for X  -1 for O\n",
    "\n",
    "    def print_board(self):\n",
    "        for row in self.board:\n",
    "            for cell in row:\n",
    "                if cell == 1:\n",
    "                    print(\"X\", end=\" \")\n",
    "                elif cell == -1:\n",
    "                    print(\"O\", end=\" \")\n",
    "                else:\n",
    "                    print(\"-\", end=\" \")\n",
    "            print()\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((3, 3))\n",
    "        self.current_player = 1\n",
    "\n",
    "    def is_board_full(self):\n",
    "        return not any(0 in row for row in self.board)\n",
    "\n",
    "    def is_winner(self, player):\n",
    "        return (\n",
    "            np.any(np.all(self.board == player, axis=0))\n",
    "            or np.any(np.all(self.board == player, axis=1))\n",
    "            or np.all(np.diag(self.board) == player)\n",
    "            or np.all(np.diag(np.fliplr(self.board)) == player)\n",
    "        )\n",
    "\n",
    "    def is_game_over(self):\n",
    "        return self.is_winner(1) or self.is_winner(-1) or self.is_board_full()\n",
    "\n",
    "    def is_tie(self):\n",
    "        return self.is_board_full() and not self.is_winner(1) and not self.is_winner(-1)\n",
    "\n",
    "    def get_available_moves(self):\n",
    "        return np.argwhere(self.board == 0)\n",
    "\n",
    "    def make_move(self, move):\n",
    "        self.board[move[0], move[1]] = self.current_player\n",
    "        self.current_player *= -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPlayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_move(self, game):\n",
    "        return game.get_available_moves()[np.random.choice(len(game.get_available_moves()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class QAgent:\n",
    "    def __init__(self, epsilon=0.1, alpha=0.5, gamma=1):\n",
    "        self.q = {}\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def getQ(self, state, action):\n",
    "        key = str((state, action))\n",
    "        # TODO check for equivalent transformations\n",
    "        if self.q.get(key) is None:\n",
    "            self.q[key] = 1.0\n",
    "        return self.q.get(key)\n",
    "\n",
    "    def updateQ(self, state, action, reward, value):\n",
    "        key = str((state, action))\n",
    "        oldv = self.q.get(key, None)\n",
    "        if oldv is None:\n",
    "            self.q[key] = reward\n",
    "        else:\n",
    "            self.q[key] = oldv + self.alpha * (value - oldv)\n",
    "\n",
    "    def choose_action(self, state, available_moves):\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            action = available_moves[np.random.randint(0, len(available_moves))]\n",
    "        else:\n",
    "            q = [self.getQ(state, a) for a in available_moves]\n",
    "            maxQ = max(q)\n",
    "            if q.count(maxQ) > 1:\n",
    "                best_options = [i for i in range(len(available_moves)) if q[i] == maxQ]\n",
    "                i = np.random.choice(best_options)\n",
    "            else:\n",
    "                i = q.index(maxQ)\n",
    "            action = available_moves[i]\n",
    "        return action\n",
    "    \n",
    "    def save_q_values(self, filename='q_values.pkl'):\n",
    "        with open(filename, 'wb') as file:\n",
    "            pickle.dump(self.q, file)\n",
    "\n",
    "\n",
    "    def load_q_values(self, filename='q_values.pkl'):\n",
    "        with open(filename, 'rb') as file:\n",
    "            self.q = pickle.load(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:17<00:00, 289.63it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def evaluate_env(environment, player):\n",
    "    score = 0\n",
    "\n",
    "    if environment.is_winner(player):\n",
    "        return 100\n",
    "\n",
    "    for i in range(3):\n",
    "        if np.count_nonzero(environment.board[i, :] == player) >= 2:\n",
    "            score += 40\n",
    "        if np.count_nonzero(environment.board[:, i] == player) >= 2:\n",
    "            score += 40\n",
    "\n",
    "    if np.count_nonzero(np.diag(environment.board) == player) >= 2:\n",
    "        score += 40\n",
    "    if np.count_nonzero(np.diag(np.fliplr(environment.board)) == player) >= 2:\n",
    "        score += 40\n",
    "\n",
    "    # if exists line with 2 opponent pieces and 1 by agent give good score\n",
    "    for i in range(3):\n",
    "        if (\n",
    "            np.count_nonzero(environment.board[i, :] == -player) == 2\n",
    "            and np.count_nonzero(environment.board[i, :] == player) == 1\n",
    "        ):\n",
    "            score += 50\n",
    "        if (\n",
    "            np.count_nonzero(environment.board[:, i] == -player) == 2\n",
    "            and np.count_nonzero(environment.board[:, i] == player) == 1\n",
    "        ):\n",
    "            score += 50\n",
    "    \n",
    "    if (\n",
    "        np.count_nonzero(np.diag(environment.board) == -player) == 2\n",
    "        and np.count_nonzero(np.diag(environment.board) == player) == 1\n",
    "    ):\n",
    "        score += 50\n",
    "    if (\n",
    "        np.count_nonzero(np.diag(np.fliplr(environment.board)) == -player) == 2\n",
    "        and np.count_nonzero(np.diag(np.fliplr(environment.board)) == player) == 1\n",
    "    ):\n",
    "        score += 50\n",
    "\n",
    "    # if exists line with 2 agent pieces and 1 empty give bad score\n",
    "    for i in range(3):\n",
    "        if (\n",
    "            np.count_nonzero(environment.board[i, :] == player) == 2\n",
    "            and np.count_nonzero(environment.board[i, :] == 0) == 1\n",
    "        ):\n",
    "            score -= 50\n",
    "        if (\n",
    "            np.count_nonzero(environment.board[:, i] == player) == 2\n",
    "            and np.count_nonzero(environment.board[:, i] == 0) == 1\n",
    "        ):\n",
    "            score -= 50\n",
    "        \n",
    "    if (\n",
    "        np.count_nonzero(np.diag(environment.board) == player) == 2\n",
    "        and np.count_nonzero(np.diag(environment.board) == 0) == 1\n",
    "    ):\n",
    "        score -= 50\n",
    "    if (\n",
    "        np.count_nonzero(np.diag(np.fliplr(environment.board)) == player) == 2\n",
    "        and np.count_nonzero(np.diag(np.fliplr(environment.board)) == 0) == 1\n",
    "    ):\n",
    "        score -= 50\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def train_QAgent(agent, environment, episodes):\n",
    "    for _ in tqdm(range(episodes)):\n",
    "        environment.reset()\n",
    "        while not environment.is_game_over():\n",
    "            available_moves = environment.get_available_moves()\n",
    "            action = agent.choose_action(environment.board, available_moves)\n",
    "\n",
    "            environment.make_move(action)\n",
    "\n",
    "            \n",
    "            if environment.is_winner(1):\n",
    "                reward = 100\n",
    "            elif environment.is_winner(-1):\n",
    "                reward = -100\n",
    "            else:\n",
    "                reward = evaluate_env(environment, environment.current_player)\n",
    "\n",
    "\n",
    "            agent.updateQ(environment.board, action, reward, reward)\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "env = TicTacToe()\n",
    "agent = QAgent()\n",
    "train_QAgent(agent, env, 5000)\n",
    "agent.save_q_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QAgent(P1) vs QAgent(P2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 656.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1 wins: 585\n",
      "P2 wins: 295\n",
      "Ties: 120\n",
      "\n",
      "QAgent(P1) vs RandomPlayer(P2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 903.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1 Wins: 582\n",
      "P2 wins: 290\n",
      "Ties: 128\n",
      "\n",
      "RandomPlayer(P1) vs QAgent(P2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 995.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1 Wins: 588\n",
      "P2 wins: 301\n",
      "Ties: 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = TicTacToe()\n",
    "agent = QAgent()\n",
    "agent.load_q_values()\n",
    "MATCHES=1000\n",
    "\n",
    "\n",
    "print(\"\\nQAgent(P1) vs QAgent(P2)\")\n",
    "wins=0\n",
    "losses=0\n",
    "ties=0\n",
    "for _ in tqdm(range(MATCHES)):\n",
    "    env.reset()\n",
    "    while not env.is_game_over():\n",
    "        action = agent.choose_action(\n",
    "            str(env.board.flatten()), env.get_available_moves()\n",
    "        )\n",
    "        env.make_move(action)\n",
    "        if env.is_winner(1):\n",
    "            wins+=1\n",
    "        if env.is_winner(-1):\n",
    "            losses+=1\n",
    "        if env.is_tie():\n",
    "            ties+=1\n",
    "\n",
    "print(f\"P1 wins: {wins}\")\n",
    "print(f\"P2 wins: {losses}\")\n",
    "print(f\"Ties: {ties}\")\n",
    "\n",
    "\n",
    "print(\"\\nQAgent(P1) vs RandomPlayer(P2)\")\n",
    "wins=0\n",
    "losses=0\n",
    "ties=0\n",
    "for _ in tqdm(range(MATCHES)):\n",
    "    env.reset()\n",
    "    while not env.is_game_over():\n",
    "        if env.current_player == 1:\n",
    "            action = agent.choose_action(\n",
    "                str(env.board.flatten()), env.get_available_moves()\n",
    "            )\n",
    "        else:\n",
    "            action = env.get_available_moves()[np.random.choice(len(env.get_available_moves()))]\n",
    "        env.make_move(action)\n",
    "        if env.is_winner(1):\n",
    "            wins+=1\n",
    "        if env.is_winner(-1):\n",
    "            losses+=1\n",
    "        if env.is_tie():\n",
    "            ties+=1\n",
    "\n",
    "print(f\"P1 Wins: {wins}\")\n",
    "print(f\"P2 wins: {losses}\")\n",
    "print(f\"Ties: {ties}\")\n",
    "\n",
    "\n",
    "print(\"\\nRandomPlayer(P1) vs QAgent(P2)\")\n",
    "wins=0\n",
    "losses=0\n",
    "ties=0\n",
    "for _ in tqdm(range(MATCHES)):\n",
    "    env.reset()\n",
    "    while not env.is_game_over():\n",
    "        if env.current_player == -1:\n",
    "            action = agent.choose_action(\n",
    "                str(env.board.flatten()), env.get_available_moves()\n",
    "            )\n",
    "        else:\n",
    "            action = env.get_available_moves()[np.random.choice(len(env.get_available_moves()))]\n",
    "        env.make_move(action)\n",
    "        if env.is_winner(1):\n",
    "            wins+=1\n",
    "        if env.is_winner(-1):\n",
    "            losses+=1\n",
    "        if env.is_tie():\n",
    "            ties+=1\n",
    "\n",
    "print(f\"P1 Wins: {wins}\")\n",
    "print(f\"P2 wins: {losses}\")\n",
    "print(f\"Ties: {ties}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
